---
title: 'Foundations for statistical inference - Confidence intervals'
author: "Emmanuel Kasigazi"
output:
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
set.seed(341124)
```

If you have access to data on an entire population, say the opinion of
every adult in the United States on whether or not they think climate
change is affecting their local community, it's straightforward to
answer questions like, "What percent of US adults think climate change
is affecting their local community?". Similarly, if you had demographic
information on the population you could examine how, if at all, this
opinion varies among young and old adults and adults with different
leanings. If you have access to only a sample of the population, as is
often the case, the task becomes more complicated. What is your best
guess for this proportion if you only have data from a small sample of
adults? This type of situation requires that you use your sample to make
inference on what your population looks like.

::: {#boxedtext}
**Setting a seed:** You will take random samples and build sampling
distributions in this lab, which means you should set a seed on top of
your lab. If this concept is new to you, review the lab on probability.
:::

## Getting Started

### Load packages

In this lab, we will explore and visualize the data using the
**tidyverse** suite of packages, and perform statistical inference using
**infer**.

Let's load the packages.

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(infer)
library(infer)
library(dplyr)
library(tinytex)
```

### The data

A 2019 Pew Research report states the following:

To keep our computation simple, we will assume a total population size
of 100,000 (even though that's smaller than the population size of all
US adults).

> Roughly six-in-ten U.S. adults (62%) say climate change is currently
> affecting their local community either a great deal or some, according
> to a new Pew Research Center survey.
>
> **Source:** [Most Americans say climate change impacts their
> community, but effects vary by
> region](https://www.pewresearch.org/fact-tank/2019/12/02/most-americans-say-climate-change-impacts-their-community-but-effects-vary-by-region/)

In this lab, you will assume this 62% is a true population proportion
and learn about how sample proportions can vary from sample to sample by
taking smaller samples from the population. We will first create our
population assuming a population size of 100,000. This means 62,000
(62%) of the adult population think climate change impacts their
community, and the remaining 38,000 does not think so.

```{r}
us_adults <- tibble(
  climate_change_affects = c(rep("Yes", 62000), rep("No", 38000))
)
```

The name of the data frame is `us_adults` and the name of the variable
that contains responses to the question *"Do you think climate change is
affecting your local community?"* is `climate_change_affects`.

We can quickly visualize the distribution of these responses using a bar
plot.

```{r bar-plot-pop, fig.height=2.5, fig.width=10}
ggplot(us_adults, aes(x = climate_change_affects)) +
  geom_bar() +
  labs(
    x = "", y = "",
    title = "Do you think climate change is affecting your local community?"
  ) +
  coord_flip() 
```

We can also obtain summary statistics to confirm we constructed the data
frame correctly.

```{r summ-stat-pop, results = TRUE}
us_adults %>%
  count(climate_change_affects) %>%
  mutate(p = n /sum(n))
```

In this lab, you'll start with a simple random sample of size 60 from
the population.

```{r sample}
n <- 60
samp <- us_adults %>%
  sample_n(size = n)
```

1.  What percent of the adults in your sample think climate change
    affects their local community? **Hint:** Just like we did with the
    population, we can calculate the proportion of those **in this
    sample** who think climate change affects their local community.

```{r thinks affects from sample}
samp %>%
  count(climate_change_affects) %>%
  mutate(p = n /sum(n))
```

```         
 climate_change_affects     n     p
```

<chr> <int> <dbl>

**1 No 25 0.417**

**2 Yes 35 0.583**

```{r pltting sample affetcs bar-plot-pop, fig.height=2.5, fig.width=10}
ggplot(samp, aes(x = climate_change_affects)) +
  geom_bar() +
  labs(
    x = "", y = "",
    title = "(sample)Do you think climate change is affecting your local community?"
  ) +
  coord_flip() 
```

1.  Would you expect another student's sample proportion to be identical
    to yours? Would you expect it to be similar? Why or why not?

**Not exactly similar in terms of figures, but they should have nearly
the same proportions because we are sampling from the same population.**

## Confidence intervals

Return for a moment to the question that first motivated this lab: based
on this sample, what can you infer about the population? With just one
sample, the best estimate of the proportion of US adults who think
climate change affects their local community would be the sample
proportion, usually denoted as $\hat{p}$ (here we are calling it
`p_hat`). That serves as a good **point estimate**, but it would be
useful to also communicate how uncertain you are of that estimate. This
uncertainty can be quantified using a **confidence interval**.

One way of calculating a confidence interval for a population proportion
is based on the Central Limit Theorem, as
$\hat{p} \pm z^\star SE_{\hat{p}}$ is, or more precisely, as
$$ \hat{p} \pm z^\star \sqrt{ \frac{\hat{p} (1-\hat{p})}{n} } $$

Another way is using simulation, or to be more specific, using
**bootstrapping**. The term **bootstrapping** comes from the phrase
"pulling oneself up by one's bootstraps", which is a metaphor for
accomplishing an impossible task without any outside help. In this case
the impossible task is estimating a population parameter (the unknown
population proportion), and we'll accomplish it using data from only the
given sample. Note that this notion of saying something about a
population parameter using only information from an observed sample is
the crux of statistical inference, it is not limited to bootstrapping.

In essence, bootstrapping assumes that there are more of observations in
the populations like the ones in the observed sample. So we
"reconstruct" the population by resampling from our sample, with
replacement. Bootstrapping is particularly useful when the sampling
distribution of the statistic is unknown or difficult to derive
theoretically. The bootstrapping scheme is as follows:

-   **Step 1.** Take a bootstrap sample - a random sample taken **with
    replacement** from the original sample, of the same size as the
    original sample.With relacement. So even though you're taking a
    sample of the same size, you're not getting the exact same data
    because: Some values might appear multiple times.Some values might
    not appear at all
-   **Step 2.** Calculate the bootstrap statistic - a statistic such as
    mean, median, proportion, slope, etc. computed on the bootstrap
    samples. This can indeed be any statistic of interest
-   **Step 3.** Repeat steps (1) and (2) many times to create a
    bootstrap distribution - a distribution of bootstrap statistics. The
    process is repeated many times (typically 1000+ times) to create the
    bootstrap distribution.
-   **Step 4.** Calculate the bounds of the XX% confidence interval as
    the middle XX% of the bootstrap distribution. Calculate the
    confidence interval bounds by finding the appropriate percentiles of
    the bootstrap distribution. For example, for a 95% confidence
    interval, you would use the 2.5th and 97.5th percentiles.

Instead of coding up each of these steps, we will construct confidence
intervals using the **infer** package.

Below is an overview of the functions we will use to construct this
confidence interval:

| Function | Purpose |
|--------------------------------------------|----------------------------|
| `specify` | Identify your variable of interest |
| `generate` | The number of samples you want to generate |
| `calculate` | The sample statistic you want to do inference with, or you can also think of this as the population parameter you want to do inference forWe use the data we get from a sample (called a sample statistic) to make educated guesses about the characteristics of the entire population (called population parameters) |
| `get_ci` | Find the confidence interval |

This code will find the 95 percent confidence interval for proportion of
US adults who think climate change affects their local community.

```{r confidence interval infer}
samp %>%
  specify(response = climate_change_affects, success = "Yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)
```

-   In `specify` we specify the `response` variable and the level of
    that variable we are calling a `success`.
-   In `generate` we provide the number of resamples we want from the
    population in the `reps` argument (this should be a reasonably large
    number) as well as the type of resampling we want to do, which is
    `"bootstrap"` in the case of constructing a confidence interval.
-   Then, we `calculate` the sample statistic of interest for each of
    these resamples, which is `prop`ortion.

Feel free to test out the rest of the arguments for these functions,
since these commands will be used together to calculate confidence
intervals and solve inference problems for the rest of the semester. But
we will also walk you through more examples in future chapters.

To recap: even though we don't know what the full population looks like,
we're 95% confident that the true proportion of US adults who think
climate change affects their local community is between the two bounds
reported as result of this pipeline.

## Confidence levels

1.  In the interpretation above, we used the phrase "95% confident".
    What does "95% confidence" mean?

    **We are relying on method hat will most likely produce the same
    same results in the long run: The 95% refers to how often the method
    works across many uses, not to any single use. We used a method (the
    bootstrap and 95% confidence level) that, in the long run, produces
    intervals that capture the true population proportion about 95% of
    the time.**

In this case, you have the rare luxury of knowing the true population
proportion (62%) since you have data on the entire population.

1.  Does your confidence interval capture the true population proportion
    of US adults who think climate change affects their local community?
    If you are working on this lab in a classroom, does your neighbor's
    interval capture this value?

    **? If we know the true population, then yes, the confidence
    interval should capture the entire population. There's a good chance
    this interval captures the true population proportion of 62%.**

2.  Each student should have gotten a slightly different confidence
    interval. What proportion of those intervals would you expect to
    capture the true population mean? Why?

**95% of the intervals would be expected to capture the true population
proportion (62% in this case) Here's why:**

**By definition, a 95% confidence interval means that if we repeated
this sampling and interval construction process many times, about 95% of
the intervals would contain the true population parameter.**

**In this case: Each student is taking a different random sample of n=60
from the same population. Each student is using that sample to construct
a 95% confidence interval using the same bootstrap method. Each interval
will likely be different because:Their initial samples of 60 are
different.Their 1000 bootstrap resamples are different**

**If we collected all these intervals: About 95% of them should contain
0.62 (the true proportion). About 5% would miss the true proportion
(either falling entirely above or below 0.62). The 95% refers to the
long-run success rate of the method, not the probability that any single
interval contains the true proportion**.

In the next part of the lab, you will collect many samples to learn more
about how sample proportions and confidence intervals constructed based
on those samples vary from one sample to another.

-   Obtain a random sample.
-   Calculate the sample proportion, and use these to calculate and
    store the lower and upper bounds of the confidence intervals.
-   Repeat these steps 50 times.

Doing this would require learning programming concepts like iteration so
that you can automate repeating running the code you've developed so far
many times to obtain many (50) confidence intervals. In order to keep
the programming simpler, we are providing the interactive app below that
basically does this for you and created a plot similar to Figure 5.6 on
[OpenIntro Statistics, 4th Edition (page
182)](https://www.openintro.org/os).

```{r shiny, echo=FALSE, eval=FALSE, results = TRUE}
# This R chunk will only run in interactive mode
store_ci <- function(i, n, reps, conf_level, success) {
  us_adults %>%
    sample_n(size = n) %>%
    specify(response = climate_change_affects, success = success) %>%
    generate(reps, type = "bootstrap") %>%
    calculate(stat = "prop") %>%
    get_ci(level = conf_level) %>%
    rename(
      x_lower = names(.)[1],
      x_upper = names(.)[2]
    )
}
library(shiny)
shinyApp(
  ui <- fluidPage(
    h4("Confidence intervals for the proportion of US adults who think 
     climate change"),

    h4(selectInput("success", "",
      choices = c(
        "is affecting their local community" = "Yes",
        "is not affecting their local community" = "No"
      ),
      selected = "Yes", width = "50%"
    )),

    # Sidebar with a slider input for number of bins
    sidebarLayout(
      sidebarPanel(
        numericInput("n_samp",
          "Sample size for a single sample from the population:",
          min = 1,
          max = 1000,
          value = 60
        ),

        hr(),

        numericInput("n_rep",
          "Number of resamples for each bootstrap confidence interval:",
          min = 1,
          max = 15000,
          value = 1000
        ),

        numericInput("conf_level",
          "Confidence level",
          min = 0.01,
          max = 0.99,
          value = 0.95,
          step = 0.05
        ),

        hr(),

        radioButtons("n_ci",
          "Number of confidence intervals:",
          choices = c(10, 25, 50, 100),
          selected = 50, inline = TRUE
        ),

        actionButton("go", "Go")
      ),

      # Show a plot of the generated distribution
      mainPanel(
        plotOutput("ci_plot")
      )
    )
  ),

  server <- function(input, output) {

    # set true p
    p <- reactive(ifelse(input$success == "Yes", 0.62, 0.38))

    # create df_ci when go button is pushed
    df_ci <- eventReactive(input$go, {
      map_dfr(1:input$n_ci, store_ci,
        n = input$n_samp,
        reps = input$n_rep, conf_level = input$conf_level,
        success = input$success
      ) %>%
        mutate(
          y_lower = 1:input$n_ci,
          y_upper = 1:input$n_ci,
          capture_p = ifelse(x_lower < p() & x_upper > p(), "Yes", "No")
        )
    })

    # plot df_ci
    output$ci_plot <- renderPlot({
      ggplot(df_ci()) +
        geom_segment(aes(x = x_lower, y = y_lower, xend = x_upper, yend = y_upper, color = capture_p)) +
        geom_point(aes(x = x_lower, y = y_lower, color = capture_p)) +
        geom_point(aes(x = x_upper, y = y_upper, color = capture_p)) +
        geom_vline(xintercept = p(), color = "darkgray") +
        labs(
          y = "", x = "Bounds of the confidence interval",
          color = "Does the interval capture the true population proportion?"
        ) +
        theme(legend.position = "bottom")
    })
  },
  options = list(height = 700)
)
```

1.  Given a sample size of 60, 1000 bootstrap samples for each interval,
    and 50 confidence intervals constructed (the default values for the
    above app), what proportion of your confidence intervals include the
    true population proportion? Is this proportion exactly equal to the
    confidence level? If not, explain why. Make sure to include your
    plot in your answer.

```{r 2, echo=FALSE}
#knitr::include_graphics("/home/legend/Documents/COMP MATH AND STATS/LABS/05_foundations_for_inference/2.png")
```

```{r 1, echo=FALSE}
#knitr::include_graphics("/home/legend/Documents/COMP MATH AND STATS/LABS/05_foundations_for_inference/1.png")
```

```{r 2a, echo=FALSE}
library(here)
#knitr::include_graphics(here("05_foundations_for_inference", "2.png"))
```

```{r 1 retry}
library(here)
#knitr::include_graphics(here("05_foundations_for_inference", "1.png"))
```

**Failed to attach the images, tried all means, must be because i use
Linux Mint. anyway I ran it a few times and used two images**

**Image 1: Has 1 red line (miss) and 49 blue lines (captures) Image 2:
Has 6 red lines (misses) and 44 blue lines (captures) So:**

**For these 50 intervals, 44/50 = 0.88 or 88% of the intervals captured
the true proportion This is not exactly equal to the 95% confidence
level**

**Why the difference?**

**Random variation: With only 50 intervals, we don't expect to see
exactly 95% capture rate. Just like flipping a coin 50 times won't
always give exactly 25 heads. Three sources of randomness here:**

**Each original sample of 60 is random Each set of 1000 bootstrap
samples is random We only have 50 intervals (small number of
replications)**

**If we were to construct many more intervals (say 10,000 instead of
50), the proportion would likely be closer to 95%. The 95% confidence
level is a long-run frequency interpretation - if we repeated this
process infinitely many times, about 95% of intervals would contain the
true proportion. In any finite sample of intervals, like our 50 here, we
expect some deviation from exactly 95%**

------------------------------------------------------------------------

## More Practice

1.  Choose a different confidence level than 95%. Would you expect a
    confidence interval at this level to me wider or narrower than the
    confidence interval you calculated at the 95% confidence level?
    Explain your reasoning.

    **I chose and 20, 45, 98 and 99 and ran each once,. ran 99 a couple
    times though Let me analyze the confidence intervals at different
    levels:**

    **Looking at widths:**

-   **99% CI (Image 1): Widest intervals**
-   **98% CI (Image 2): Very wide, but slightly narrower than 99%**
-   **80% CI (Image 3): Narrower intervals**
-   **45% CI (Image 4): Even narrower**
-   **20% CI (Image 5): Narrowest intervals**

**The pattern shows: As confidence level decreases, the intervals become
narrower.**

**Why this happens: 1. Higher confidence (like 99% or 98%): - Needs to
"catch" the true parameter more often - Must be wider to have a higher
chance of containing the true value - Trades precision for confidence**

2.  **Lower confidence (like 45% or 20%):**
    -   **Accepts missing the true parameter more often**
    -   **Can be narrower since it doesn't need to capture the parameter
        as frequently**
    -   **Trades confidence for precision**

**Am personally Thinking of it like a net:**

**- A wide net (high confidence level) catches more fish but is less
precise**

**- A narrow net (low confidence level) is more precise but misses more
fish**

**This pattern illustrates the fundamental trade-off in confidence
intervals: higher confidence requires wider intervals, while narrower
intervals come with lower confidence.**

1.  Using code from the **infer** package and data fromt the one sample
    you have (`samp`), find a confidence interval for the proportion of
    US Adults who think climate change is affecting their local
    community with a confidence level of your choosing (other than 95%)
    and interpret it.

```{r Using the infer package for a 90% confidence interval}
# Using the infer package for a 90% confidence interval
samp %>%
  specify(response = climate_change_affects, success = "Yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.90)  # Changed from 0.95 to 0.90
```

**This 90% confidence interval for the proportion of US adults who think
climate change affects their local community is (0.55, 0.75) or between
55% and 75%. A few observations:**

**The true population proportion (0.62 or 62%) is contained within this
interval.**

**The interval width is 0.75 - 0.55 = 0.20 (or 20 percentage points)**

**The interval is not symmetric around the sample proportion (this is
normal for proportion confidence intervals)**

**This interval is likely narrower than a 95% confidence interval would
be for the same data, since:**

**A 90% confidence level means we're willing to be "wrong" 10% of the
time (vs 5% for a 95% CI)**

**This allows us to create a more precise (narrower) interval But we
have less confidence that this interval contains the true population
proportion**

1.  Using the app, calculate 50 confidence intervals at the confidence
    level you chose in the previous question, and plot all intervals on
    one plot, and calculate the proportion of intervals that include the
    true population proportion. How does this percentage compare to the
    confidence level selected for the intervals?

**Still failing to attach images , but Counting the intervals:**

**Total intervals: 50 Intervals in blue (capturing true proportion of
0.62): \~45 Intervals in red (missing true proportion): \~5**

**Calculating capture percentage:**

**Approximately 45/50 = 0.90 or 90% of intervals captured the true
proportion This matches almost exactly with our chosen 90% confidence
level**

**This is a great demonstration of what a confidence level means in
practice:**

**We set our confidence level at 90% We constructed 50 different
intervals About 90% of them (45) captured the true population proportion
About 10% of them (5) missed it**

**The observed capture rate (90%) aligns remarkably well with the chosen
confidence level (90%) in this case. However, it's important to note:**

**This close match is somewhat lucky - with only 50 intervals, we could
easily see more variation In the long run (with many more intervals),
we'd expect the capture rate to converge to 90% Each individual interval
still either contains or doesn't contain the true proportion - the 90%
refers to the long-run behavior of the method**

1.  Lastly, try one more (different) confidence level. First, state how
    you expect the width of this interval to compare to previous ones
    you calculated. Then, calculate the bounds of the interval using the
    **infer** package and data from `samp` and interpret it. Finally,
    use the app to generate many intervals and calculate the proportion
    of intervals that are capture the true population proportion.

    ```{r Using the infer package for a 80% confidence interval}
    # Using the infer package for a 80% confidence interval
    samp %>%
      specify(response = climate_change_affects, success = "Yes") %>%
      generate(reps = 1000, type = "bootstrap") %>%
      calculate(stat = "prop") %>%
      get_ci(level = 0.80)  # Changed from 0.90 to 0.80
    ```

    **Expected width comparison: An 80% confidence interval should be
    narrower than both the 95% and 90% intervals we calculated earlier.
    This is because with lower confidence (80%), we're willing to be
    "wrong" more often (20% of the time), allowing us to make more
    precise (narrower) estimates**

lower_ci upper_ci <dbl> <dbl>

1 0.567 0.733

**Analyzing the calculated interval (0.567, 0.733):**

**The interval suggests we're 80% confident that the true proportion is
between 56.7% and 73.3% Width = 0.733 - 0.567 = 0.166 (or 16.6
percentage points) This is indeed narrower than our previous 90% CI
(0.55, 0.75) which had a width of 0.20 (20 percentage points) The true
proportion (0.62) is contained within this interval**

**Looking at the simulation from the app (seen in the shiny app):**

**Total intervals: 50 Blue lines (captures): \~44-45 Red lines (misses):
\~5-6 Capture rate: \~44/50 = 88% This capture rate is actually higher
than our chosen 80% confidence level The difference (88% vs 80%) is
likely due to random variation in this particular set of simulations -
with more runs, we'd expect the capture rate to converge closer to 80%**

**The key point here is that while we got a narrower interval with 80%
confidence as expected, the actual capture rate in our simulation was a
bit higher than 80%, demonstrating that results from a finite number of
trials can differ from the theoretical expectation**.

1.  Using the app, experiment with different sample sizes and comment on
    how the widths of intervals change as sample size changes (increases
    and decreases). **Higher confidence intervals are wider because they
    need to capture the true parameter more often—like using a wide net
    to catch more fish. In contrast, lower confidence intervals are
    narrower (more precise), but they’re more likely to miss the true
    value. This illustrates the trade-off: as you increase confidence,
    you sacrifice precision, and vice versa.**

**Insert your answer here**

1.  Finally, given a sample size (say, 60), how does the width of the
    interval change as you increase the number of bootstrap samples.
    **Hint:** Does changing the number of bootstap samples affect the
    standard error?

```{r many bootstarp sampples b}
# With 100 resamples
samp %>%
  specify(response = climate_change_affects, success = "Yes") %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)

# With 1000 resamples
samp %>%
  specify(response = climate_change_affects, success = "Yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)

# With 10000 resamples
samp %>%
  specify(response = climate_change_affects, success = "Yes") %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)
```

**The key insight is that increasing the number of bootstrap samples
(reps) does NOT substantially affect the width of the confidence
interval because:**

**Bootstrap resampling is about better estimating the sampling
distribution It doesn't change the underlying sample size (n=60)**

**The standard error primarily depends on the original sample size, not
the number of bootstrap replicates**

**The width of these intervals would be very similar because:**

**Standard error depends on √(p(1-p)/n) where n is the original sample
size. More bootstrap samples just give us a more stable estimate of this
same standard error. The fundamental uncertainty in our estimate comes
from our original sample size, not from how many times we resample it**

**The main benefit of more bootstrap samples is:**

**More stable estimates Smoother sampling distributions. More precise
percentile calculations. But it doesn't reduce the width of the interval
in any meaningful way.**

------------------------------------------------------------------------
